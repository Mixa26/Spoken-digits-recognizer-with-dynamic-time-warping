{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":7176598,"sourceType":"datasetVersion","datasetId":4147299},{"sourceId":7177272,"sourceType":"datasetVersion","datasetId":4147816}],"dockerImageVersionId":30615,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Library importations","metadata":{}},{"cell_type":"code","source":"pip install fastdtw","metadata":{"execution":{"iopub.status.busy":"2023-12-12T09:15:31.137747Z","iopub.execute_input":"2023-12-12T09:15:31.138169Z","iopub.status.idle":"2023-12-12T09:16:02.991593Z","shell.execute_reply.started":"2023-12-12T09:15:31.138133Z","shell.execute_reply":"2023-12-12T09:16:02.990264Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting fastdtw\n  Downloading fastdtw-0.3.4.tar.gz (133 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.4/133.4 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from fastdtw) (1.24.3)\nBuilding wheels for collected packages: fastdtw\n  Building wheel for fastdtw (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for fastdtw: filename=fastdtw-0.3.4-cp310-cp310-linux_x86_64.whl size=118376 sha256=1f772a9d9303b344b4fda815d34dba9ba553da7a33da8d2da1f1eb67499c61bc\n  Stored in directory: /root/.cache/pip/wheels/73/c8/f7/c25448dab74c3acf4848bc25d513c736bb93910277e1528ef4\nSuccessfully built fastdtw\nInstalling collected packages: fastdtw\nSuccessfully installed fastdtw-0.3.4\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\nimport librosa\nfrom fastdtw import fastdtw\nfrom scipy.spatial.distance import euclidean\nfrom scipy.spatial.distance import cdist\nimport matplotlib.pyplot as plt\nimport seaborn as sns","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-12-12T12:53:43.641744Z","iopub.execute_input":"2023-12-12T12:53:43.642200Z","iopub.status.idle":"2023-12-12T12:53:43.648876Z","shell.execute_reply.started":"2023-12-12T12:53:43.642166Z","shell.execute_reply":"2023-12-12T12:53:43.647699Z"},"trusted":true},"execution_count":52,"outputs":[]},{"cell_type":"markdown","source":"## 1. Loading digit pronunciations for references","metadata":{}},{"cell_type":"code","source":"def load_digits(path):\n    zero, sr = librosa.load('{}/0-55-20.wav'.format(path))\n    one, sr = librosa.load('{}/1-55-20.wav')\n    two, sr = librosa.load('{}/2-55-20.wav')\n    three, sr = librosa.load('{}/3-55-20.wav')\n    four, sr = librosa.load('{}/4-55-20.wav')\n    five, sr = librosa.load('{}/5-55-20.wav')\n    six, sr = librosa.load('{}/6-55-20.wav')\n    seven, sr = librosa.load('{}/7-55-20.wav')\n    eight, sr = librosa.load('{}/8-55-20.wav')\n    nine, sr = librosa.load('{}/9-55-20.wav')\n    \n    return zero, one, two, three, four, five, six, seven, eight, nine","metadata":{"execution":{"iopub.status.busy":"2023-12-12T11:24:56.568838Z","iopub.execute_input":"2023-12-12T11:24:56.569680Z","iopub.status.idle":"2023-12-12T11:24:56.579767Z","shell.execute_reply.started":"2023-12-12T11:24:56.569635Z","shell.execute_reply":"2023-12-12T11:24:56.578560Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"digits = load_digits('/kaggle/input/vocal-digits/digits')","metadata":{"execution":{"iopub.status.busy":"2023-12-12T11:24:57.985966Z","iopub.execute_input":"2023-12-12T11:24:57.986378Z","iopub.status.idle":"2023-12-12T11:24:58.932501Z","shell.execute_reply.started":"2023-12-12T11:24:57.986348Z","shell.execute_reply":"2023-12-12T11:24:58.931564Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"## 2. Dynamic Time Warping (DTW) spoken digit detection","metadata":{}},{"cell_type":"code","source":"def distance_matrix(signal1, signal2):   \n    return cdist(signal1, signal2, \"cosine\")","metadata":{"execution":{"iopub.status.busy":"2023-12-12T12:54:39.635854Z","iopub.execute_input":"2023-12-12T12:54:39.636302Z","iopub.status.idle":"2023-12-12T12:54:39.641908Z","shell.execute_reply.started":"2023-12-12T12:54:39.636269Z","shell.execute_reply":"2023-12-12T12:54:39.640995Z"},"trusted":true},"execution_count":53,"outputs":[]},{"cell_type":"code","source":"def dtw(signal1, signal2):\n    distance = distance_matrix(signal1, signal2)\n    \n    NCOA = 0 #Normalized cost of alignment\n\n    cost = np.zeros((signal1.shape[0]+1, signal2.shape[0]+1))\n    \n    cost[:,0] = np.inf\n    cost[signal1.shape[0],:] = np.inf\n    cost[signal1.shape[0],0] = 0\n    \n    traceback_matrix = np.zeros((signal1.shape[0]+1, signal2.shape[0]+1))\n    for i in range(signal1.shape[0], -1, -1):\n        for j in range(signal2.shape[0]):\n            curr_cost = [cost[i+1][j], cost[i+1][j+1], cost[i][j]]\n                        #match,        insertion,      deletion\n            action_index = np.argmin(curr_cost)\n            traceback_matrix[i][j] = action_index\n            cost[i][j+1] = curr_cost[action_index] + distance[i][j]\n            \n    path = [(i,j)]\n    while i < signal1.shape[0]+1 or j < signal2.shape[0]+1:\n        if traceback_matrix[i][j] == 0:\n            #match\n            i -= 1\n            j += 1\n        elif traceback_matrix[i][j] == 1:\n            #insert\n            i -= 1\n        else:\n            #delete\n            j += 1\n        NCOA += cost[i][j]\n        path.append((i,j))\n        \n    return NCOA, path[1:], cost[:-1, 1:]","metadata":{"execution":{"iopub.status.busy":"2023-12-12T12:07:59.578853Z","iopub.execute_input":"2023-12-12T12:07:59.579300Z","iopub.status.idle":"2023-12-12T12:07:59.592561Z","shell.execute_reply.started":"2023-12-12T12:07:59.579265Z","shell.execute_reply":"2023-12-12T12:07:59.590882Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"def detect_digit(audio_path):\n    #Preprocess the input audio\n    audio, sr = librosa.load(audio_path)\n       \n    #Distances to each number pronunciation\n    distances_mfcc = []\n    mfcc_score = 0\n    distances_ms = []\n    ms_score = 0\n    distances_lpc = []\n    lpc_score = 0\n    #Threshold for detecting if the audio symbolizes a digit\n    digit_threshold = 1\n    \n    for index, digit in enumerate(digits):\n        #distance, path, cost = dtw(audio, digit)\n        #MFCC approach\n        digit_mfcc = librosa.feature.mfcc(y=digit, n_mfcc=40).T\n        audio_mfcc = librosa.feature.mfcc(y=audio, n_mfcc=40).T\n        distance, path = fastdtw(audio_mfcc, digit_mfcc, dist=euclidean)\n        distances_mfcc.append(distance / len(path))\n        if np.argmin(np.asarray(distances_mfcc)) == index:\n            mfcc_score += 1\n        #MS approach\n        digit_ms = np.log(librosa.feature.melspectrogram(y=digit, n_mels=40)).T\n        audio_ms = np.log(librosa.feature.melspectrogram(y=audio, n_mels=40)).T\n        distance, path = fastdtw(audio_ms, digit_ms, dist=euclidean)\n        distances_ms.append(distance / len(path))\n        if np.argmin(np.asarray(distances_ms)) == index:\n            ms_score += 1 \n        #LPC approach\n        digit_lpc = librosa.lpc(y=digit, order=40)\n        audio_lpc = librosa.lpc(y=audio, order=40)\n        distance, path = fastdtw(audio_lpc, digit_lpc)\n        distances_lpc.append(distance / len(path))\n        if np.argmin(np.asarray(distances_lpc)) == index:\n            lpc_score += 1 \n        \n    #We search for the minimum distance (most likely digit)\n    prediction_index = np.argmin(np.asarray(distances_lpc))\n    \n    #Check if the audio is a digit based on the threshold\n    if distances_lpc[prediction_index] > digit_threshold:\n        print(\"The audio doesn't symbolize a digit\\n\")\n    else:\n        print(\"The audio is a digit, most likely the number {}\\n\".format(prediction_index))\n\n    print(\"Normalized cost of alignment scores:\")\n    print(\"For numbers: {}\".format(\" \".join([\"{:<5}\".format(i) for i in range(10)])))\n    print(\"MFCC:        {}\".format(\" \".join(\"{:.2f}\".format(distances_mfcc[i]) for i in range(10))))\n    print(\"MS:          {}\".format(\" \".join(\"{:.2f}\".format(distances_ms[i]) for i in range(10))))\n    print(\"LPC:         {}\".format(\" \".join(\"{:.2f}\".format(distances_lpc[i]) for i in range(10))))\n    print(\"Correct guesses for MFC: {}, MS: {}, LPC: {}\".format(mfcc_score, ms_score, lpc_score))","metadata":{"execution":{"iopub.status.busy":"2023-12-12T13:51:05.663245Z","iopub.execute_input":"2023-12-12T13:51:05.663832Z","iopub.status.idle":"2023-12-12T13:51:05.684142Z","shell.execute_reply.started":"2023-12-12T13:51:05.663770Z","shell.execute_reply":"2023-12-12T13:51:05.682666Z"},"trusted":true},"execution_count":150,"outputs":[]},{"cell_type":"code","source":"detect_digit('/kaggle/input/test-digits/test_digits/3-51-20.wav')","metadata":{"execution":{"iopub.status.busy":"2023-12-12T13:50:03.178521Z","iopub.execute_input":"2023-12-12T13:50:03.179863Z","iopub.status.idle":"2023-12-12T13:50:04.882107Z","shell.execute_reply.started":"2023-12-12T13:50:03.179807Z","shell.execute_reply":"2023-12-12T13:50:04.879965Z"},"trusted":true},"execution_count":149,"outputs":[{"name":"stdout","text":"The audio is a digit, most likely the number 3\n\nNormalized cost of alignment scores:\nFor numbers: 0     1     2     3     4     5     6     7     8     9    \nMFCC:        82.6 77.2 79.4 75.0 81.3 78.0 81.3 78.8 75.8 77.5\nMS:          13.8 13.1 13.6 13.0 13.5 13.2 13.5 13.4 13.3 13.3\nLPC:         0.6 0.9 0.6 0.7 1.0 0.9 0.6 0.4 0.2 0.8\nCorrect guesses for MFC: 3, MS: 3, LPC: 4\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## 3. Find the vocally closest person","metadata":{}},{"cell_type":"code","source":"def find_closest_vocal(path):\n    person_dict = {}\n\n    distances_lpc = {}\n    closest_person = ''\n    closest_person_min = np.inf\n    \n    for filename in os.listdir(path):\n        if filename.endswith(\".wav\"):\n            person_identifier = filename.split('-')[1] + \"-\" + filename.split('-')[2].split('.')[0]\n\n            if person_identifier not in person_dict:\n                person_dict[person_identifier] = [librosa.load(path + \"/\" + filename)[0]]\n            else:\n                person_dict[person_identifier].append(librosa.load(path + \"/\" + filename)[0])\n\n    for person, files in person_dict.items():\n        for i in range(10):\n            #LPC approach\n            digit_lpc = librosa.lpc(y=digits[i], order=40)\n            audio_lpc = librosa.lpc(y=files[i], order=40)\n            distance, path = fastdtw(audio_lpc, digit_lpc)\n            if i == 0:\n                distances_lpc[person] = distance / len(path)\n            else:\n                distances_lpc[person] += distance / len(path)\n        if distances_lpc[person] < closest_person_min:\n            closest_person = person\n                \n    print(\"The closest person vocally to you has the index: {}\".format(closest_person))","metadata":{"execution":{"iopub.status.busy":"2023-12-12T14:23:06.414308Z","iopub.execute_input":"2023-12-12T14:23:06.414737Z","iopub.status.idle":"2023-12-12T14:23:06.428787Z","shell.execute_reply.started":"2023-12-12T14:23:06.414703Z","shell.execute_reply":"2023-12-12T14:23:06.427852Z"},"trusted":true},"execution_count":159,"outputs":[]},{"cell_type":"code","source":"find_closest_vocal('/kaggle/input/test-digits/test_digits')","metadata":{"execution":{"iopub.status.busy":"2023-12-12T14:23:09.480488Z","iopub.execute_input":"2023-12-12T14:23:09.481468Z","iopub.status.idle":"2023-12-12T14:23:10.157743Z","shell.execute_reply.started":"2023-12-12T14:23:09.481381Z","shell.execute_reply":"2023-12-12T14:23:10.156294Z"},"trusted":true},"execution_count":160,"outputs":[{"name":"stdout","text":"The closest person vocally to you has the index: 51-20\n","output_type":"stream"}]},{"cell_type":"markdown","source":"The strategy we used above to detect the vocally closest person is the following.<br>\nWe go through all the test data (test_digits) and we compare each of our spoken digits to each of another persons spoken digits<br>\nby calculating the costs between them using DTW. For preprocessing we used LPC because we saw the best results with it while<br>\ndetecting numbers.<br>\nWe calculate normalized cost of alignment (NCOA) for each digit and add them all up together for each person.<br>\nNCOA is calculated as the total_cost_of_path/length_of_path.<br>\nAt least we just look at which person has the minimum sum because that means that person has the closest pronunciation to us.","metadata":{}},{"cell_type":"markdown","source":"## 4. DTW Pruning","metadata":{}},{"cell_type":"markdown","source":"As we saw in this project DTW can be very computationally expensive, it's time complaxity is quadratic which can be disastrous for bigger sequences of data. To address this issue a couple of pruning strategies have been advised as solutions such as:<br>\n\n### Sakoe-Chiba Band:\n\nThe Sakoe-Chiba band is a restriction on the alignment path, limiting the allowed deviations in the warping path. It introduces a constraint that only allows points within a certain diagonal band to be matched, excluding points outside this band. This significantly reduces the number of possible alignments.\n\n### Itakura Parallelogram:\n\nThe Itakura parallelogram is another constraint applied during DTW. It limits the allowed warping path to a parallelogram shape, which is especially useful for speech recognition applications. Similar to the Sakoe-Chiba band, this constraint reduces the search space and computational complexity.\n\n### Early Abandoning:\n\nEarly abandoning involves terminating the DTW computation early if it becomes evident that the current path is already worse than the best path found so far. This strategy relies on maintaining a lower bound on the distance during computation and abandoning the alignment if the lower bound exceeds the best distance found.\n\n### Pruning by Threshold:\n\nPruning by threshold involves setting a distance threshold, below which the algorithm stops the computation of certain alignments. If the accumulated distance exceeds the threshold, the alignment is pruned, reducing the number of comparisons.\n\n### Windowing:\n\nWindowing involves restricting the search space to a specific region around the diagonal. Only points within a certain window are considered for alignment, excluding points outside the window. This is a less strict constraint compared to the Sakoe-Chiba band but still reduces the number of calculations.","metadata":{}}]}